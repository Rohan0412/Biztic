{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20428439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import sqlite3\n",
    "import csv\n",
    "from lxml import etree\n",
    "from lxml.html.clean import clean_html\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf4f2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.ndtv.com/'\n",
    "\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df344fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"html.parser\") #initializing parser\n",
    "# soup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22c75f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dom = etree.HTML(str(soup)) \n",
    "\n",
    "list_title = dom.xpath('//div[@class=\"featured_cont\"]/span/h2/span/a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1280725a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Stories \n"
     ]
    }
   ],
   "source": [
    "for title in list_title:\n",
    "    print(title.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5eec59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BMW Used In Former Model's Murder Found In Punjab, Body Still Missing\n",
      "Teen Gunman Kills Fellow Student, Injures 5 At US School, Then Shoots Self\n",
      "Court Allows Woman To End Pregnancy Over Trauma After Husband's Death\n",
      "\"Sorry, You're Not A Batter If...\": Gavaskar's Sharp Attack Post 2nd Test\n",
      "\"Won't Be Surprising If He Is Arrested\": Sharad Pawar On Arvind Kejriwal\n",
      "Israeli Defence Minister Unveils Plan For Post-War Gaza\n",
      "\"Be Neutral\": Rohit Slams ICC Match Referees, Indicates Bias Against India\n",
      "Chinese State Media Praises India's Growth, Foreign Policy Under PM Modi\n",
      "4 Properties Owned By Dawood Up For Bidding Today At Just Rs 19 Lakh\n",
      "Save A Life. Donate A Blanket\n"
     ]
    }
   ],
   "source": [
    "list_li = dom.xpath('//span[@id=\"topstoriesdata\"]/ul/li/h2/a[2]')\n",
    "\n",
    "\n",
    "for li in list_li:\n",
    "    print(li.text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018f0df",
   "metadata": {},
   "source": [
    "# Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e59a7c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  None\n",
      "Text:  None\n",
      "Text:  None\n",
      "Text:  None\n",
      "Text:  None\n",
      "Text:  None\n",
      "Text:  None\n",
      "Text:  None\n",
      "Text:  None\n",
      "Text:  None\n",
      "Text:  None\n",
      "Text:  None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "\n",
    "def get_all_links(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            html_code = response.text\n",
    "            soup = BeautifulSoup(html_code, 'html.parser')\n",
    "\n",
    "            dom = etree.HTML(str(soup))\n",
    "\n",
    "            \n",
    "            # Use XPath to get the desired elements\n",
    "            list_titles = dom.xpath(\"//div[@class='container']/ul/li/a/span/strong\")\n",
    "\n",
    "            # Iterate over the extracted elements and print or process them\n",
    "            for title in list_titles:\n",
    "                print(\"Text: \", title.text)\n",
    "#                 print(\"Link: \", title.get('href'))\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch the product page. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "main_url = 'https://mdcomputers.in'\n",
    "get_all_links(main_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c96429",
   "metadata": {},
   "source": [
    "# 1 Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa1c7c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Not available\n",
      "Text: Not available\n",
      "Text: Not available\n",
      "Text: Not available\n",
      "Text: Not available\n",
      "Text: Not available\n",
      "Text: Not available\n",
      "Text: Not available\n",
      "Text: Not available\n",
      "Text: Not available\n",
      "Text: Not available\n",
      "Text: Not available\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "\n",
    "def get_all_links(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            html_code = response.text\n",
    "            soup = BeautifulSoup(html_code, 'html.parser')\n",
    "\n",
    "            dom = etree.HTML(str(soup))\n",
    "\n",
    "            # Adjusted XPath to target the correct elements\n",
    "            list_titles = dom.xpath(\"//div[@class='container']/ul/li/a/span/strong\")\n",
    "\n",
    "            # Iterate over the extracted elements and print or process them\n",
    "            for title in list_titles:\n",
    "                # Check if title.text is None before printing\n",
    "                if title.text is not None:\n",
    "                    print(\"Text: \", title.text)\n",
    "                else:\n",
    "                    print(\"Text: Not available\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch the product page. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "main_url = 'https://mdcomputers.in'\n",
    "get_all_links(main_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65116fe3",
   "metadata": {},
   "source": [
    "# 2 Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04fde355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  Custom Cooling System\n",
      "Text:  Processor\n",
      "Text:  Cooling System\n",
      "Text:  Motherboard\n",
      "Text:  Memory (Ram)\n",
      "Text:  Storage\n",
      "Text:  Graphics Card\n",
      "Text:  Power Supply\n",
      "Text:  Cabinet (Case)\n",
      "Text:  Monitors\n",
      "Text:  Peripherals\n",
      "Text:  Combo Deals\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "\n",
    "def get_all_links(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            html_code = response.text\n",
    "            soup = BeautifulSoup(html_code, 'html.parser')\n",
    "\n",
    "            dom = etree.HTML(str(soup))\n",
    "\n",
    "            # Use XPath to get the desired elements\n",
    "            list_titles = dom.xpath(\"//div[@class='container']/ul/li/a/span/strong/text()\")\n",
    "\n",
    "            # Iterate over the extracted elements and print or process them\n",
    "            for title in list_titles:\n",
    "                print(\"Text: \", title.strip())\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch the product page. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "main_url = 'https://mdcomputers.in'\n",
    "get_all_links(main_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a825bc",
   "metadata": {},
   "source": [
    "# Proper Final Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73091715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import sqlite3\n",
    "import csv\n",
    "from lxml import etree\n",
    "\n",
    "import mysql.connector\n",
    "\n",
    "def store_in_database(product_data_list):\n",
    "    conn = mysql.connector.connect(user='root', password='1234',\n",
    "                              host='127.0.0.1', database='md',\n",
    "                              auth_plugin='mysql_native_password')    \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    for product_data in product_data_list:    \n",
    "        cursor.execute(''' INSERT INTO products (title, url, price_old, price_new, discount,created_at,updated_at,category) VALUES (%s,%s,%s,%s,%s,now(),now(),%s) ''',( product_data[0], product_data[1], product_data[2],   product_data[3], product_data[4],product_data[5]))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def get_html_code(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# def get_all_links(html_code,url):\n",
    "        \n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "\n",
    "    dom = etree.HTML(str(soup))\n",
    "\n",
    "    list_titles = dom.xpath(\"//ul[@class='megamenu']/li/a\")\n",
    "    \n",
    "    main_links = []\n",
    "\n",
    "    for title in list_titles:\n",
    "        product_urls = urljoin(url,title.get('href'))\n",
    "        main_links.append(product_urls)\n",
    "\n",
    "    if not product_urls:\n",
    "        print(\"No product links found.\")\n",
    "    \n",
    "    return main_links\n",
    "\n",
    "\n",
    "def get_all_links(html_code, url):\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "    dom = etree.HTML(str(soup))\n",
    "\n",
    "    list_titles = dom.xpath(\"//ul[@class='megamenu']/li/a\")\n",
    "\n",
    "    main_links = []\n",
    "\n",
    "    for title in list_titles:\n",
    "        product_urls = urljoin(url,title.get('href'))\n",
    "        main_links.append(product_urls)\n",
    "\n",
    "    if not main_links:\n",
    "        print(\"No product links found.\")\n",
    "\n",
    "    return main_links\n",
    "\n",
    "\n",
    "def scrape_product_details(url):\n",
    "    print(\"get document for :\",url)\n",
    "    \n",
    "    next_page_url=''\n",
    "\n",
    "    pattern = re.compile('(?:.*?)/(.*?)(?:\\\\?|$)')\n",
    "    match = re.search(pattern, url)\n",
    "\n",
    "    if match:\n",
    "        category = match.group(1)\n",
    "        # Remove incremental values (if any)\n",
    "        category = re.sub(r'\\d+$', '', category)\n",
    "\n",
    "    category = category.replace('/mdcomputers.in/','')\n",
    "\n",
    "    html_code = get_html_code(url)\n",
    "    \n",
    "    product_data_list = []\n",
    "    \n",
    "    if html_code:\n",
    "        soup = BeautifulSoup(html_code, 'html.parser')\n",
    "        \n",
    "        dom = etree.HTML(str(soup))\n",
    "\n",
    "        product_list = dom.xpath(\"//div[@class='products-list row nopadding-xs']/div/div\")\n",
    "\n",
    "        for product in product_list:\n",
    "            title_elem = product.xpath(\".//div[@class='right-block right-b']/h4/a\")            \n",
    "            title = title_elem[0].text.strip() \n",
    "            product_url_elem = product.xpath(\".//div[@class='right-block right-b']/h4/a\")\n",
    "            product_url = product_url_elem[0].get('href').strip() if product_url_elem else 'URL not found'\n",
    "            price_old_elem = product.xpath(\".//span[@class='price-old']\")\n",
    "            price_old = price_old_elem[0].text.strip() if price_old_elem else 'Old Price not found'\n",
    "            price_new_elem = product.xpath(\".//span[@class='price-new']\")\n",
    "            price_new = price_new_elem[0].text.strip()if price_new_elem else 'New Price not found'\n",
    "            discount_elem =  product.xpath(\".//span[@class='label-product label-sale']\") \n",
    "            discount = discount_elem[0].text.strip() if discount_elem else 'Discount not found'            \n",
    "            # print(f\"Title: {title}\")\n",
    "            # print(f\"URL: {product_url}\")\n",
    "            # print(f\"Old Price: {price_old}\")\n",
    "            # print(f\"New Price: {price_new}\")\n",
    "            # print(f\"Discount: {discount}\")\n",
    "            product_data = (title, product_url, price_old, price_new, discount,category)\n",
    "            product_data_list.append(product_data)\n",
    "                       \n",
    "        store_in_database(product_data_list)\n",
    "           \n",
    "        next_page_node = dom.xpath(\".//ul[@class='pagination']/li/a\")    \n",
    "        \n",
    "        for next_link in next_page_node:\n",
    "            if next_link.text == '>':\n",
    "               next_page_url = next_link.get('href')\n",
    "        \n",
    "        if next_page_url!= '':\n",
    "            scrape_product_details(next_page_url)\n",
    "            \n",
    "    else:\n",
    "        print(\"Error: HTML code is not available.\")        \n",
    "    \n",
    "\n",
    "def scrape_all_details(url):\n",
    "    html_code = get_html_code(url)\n",
    "    if html_code:\n",
    "        product_urls = get_all_links(html_code,url)\n",
    "        for product_url in product_urls:                \n",
    "            scrape_product_details(product_url)    \n",
    "            \n",
    "\n",
    "main_url = 'https://mdcomputers.in'\n",
    "scrape_all_details(main_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
